{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/alsayedhamdy/text-clustering-and-feature-engineering?scriptVersionId=107567848\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# **Importing Libraries**","metadata":{"id":"3pP8pmEiUhOV"}},{"cell_type":"code","source":"#Importing libraries\nimport numpy as np\nfrom numpy import unique\nfrom numpy import where\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nfrom wordcloud import WordCloud\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import model_selection, metrics, preprocessing, ensemble, model_selection, metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score, silhouette_samples\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import cross_val_score\nfrom hyperopt import tpe, hp, fmin, STATUS_OK,Trials\nfrom hyperopt.pyll.base import scope\nimport nltk\nimport time\nimport re\nimport string\nfrom kmeanstf import KMeansTF\nfrom nltk.tokenize import word_tokenize \nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom string import punctuation\nfrom gensim.utils import tokenize","metadata":{"id":"2GoP3FxMYinH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')","metadata":{"id":"cLeePC64gKct","outputId":"5ecbd946-799a-45b3-e854-1b3db0f2af26"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data editing and understanding**","metadata":{"id":"uklVgqNNU9l7"}},{"cell_type":"markdown","source":"**Upload and understand the data**","metadata":{"id":"bP8hA1ihU1oN"}},{"cell_type":"code","source":"df = pd.read_csv('/content/Indiegogo.csv')\ndf","metadata":{"id":"YqnIAatDagg7","outputId":"235cee51-4e2d-410b-b912-b2ada69d09cd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"bwua2-aYb2jY","outputId":"0ed730b5-d068-45c3-f31c-68758cb92d1e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.   **I will ignore all the features except the feature ('title')**\n2.   **And I will decrease the number of raws as well so the data won't be so huge**","metadata":{"id":"a4wR9HNfVUqi"}},{"cell_type":"code","source":"df = df.drop(range(15100, 32321))\ndf = df.drop(df.loc[:, 'bullet_point':'tags'].columns, axis=1)\ndf","metadata":{"id":"JiOchz5AcLq8","outputId":"5f1dbbd8-50e6-4e8a-9557-8c7ad16271c5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now I will look for the missing values and clear it**","metadata":{"id":"BrDP_3JeWMno"}},{"cell_type":"code","source":"df.describe()","metadata":{"id":"RzX52dq8dRnT","outputId":"03006377-69be-4b22-b399-e37296e1670c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"id":"ISSpYBOefHRl","outputId":"dacd1b44-0a59-4f62-d2c2-eb9774cedaba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.dropna()","metadata":{"id":"cx6xA17TfMdl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"id":"hV6ZiGOdfd39","outputId":"d627d54d-cf3f-40f7-8dcb-408f57442dde"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reset the index so it won't be any confusion\ndf.reset_index(inplace=True)\ndf","metadata":{"id":"pWlSd94Jfijt","outputId":"5ec15b08-26be-45d2-98d2-23c6fbf1ada6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data preprocessing and feature engineering**","metadata":{"id":"RLg2wsRSWhbA"}},{"cell_type":"code","source":"X = df['title']","metadata":{"id":"lD4Shd1UgAqP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now I will clean the text and do the data preprocessing**","metadata":{"id":"4JLpC1KMWubw"}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nps = SnowballStemmer('english')\n#Let's make a function to finish all the cleaning process\ndef clean_data(text):\n    #Removing URLs\n    text = re.sub('http\\S+\\s*', ' ', text)\n    #Removing RT and cc\n    text = re.sub('RT|cc', ' ', text)\n    #Removing digits\n    text = re.sub(r'\\d+', '', text)\n    #Removing hashtags\n    text = re.sub('#\\S+', '', text)\n    #Removing mentions and E-mails\n    text = re.sub('@\\S+', '  ', text) \n    #Removing punctuations\n    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', text) \n    #Replacing non-ASCII characters with a single space\n    text = re.sub(r'[^\\x00-\\x7f]',r' ', text) \n    #Removing extra whitespace\n    text = re.sub('\\s+', ' ', text)\n    #Making the text in lowercase\n    text = \"\".join([char.lower() for char in text if char not in string.punctuation])\n    #Removing stopwords\n    text = \" \".join([word for word in str(text).split() if word not in stop_words])\n    #Stemming\n    text = \" \".join([ps.stem(word) for word in text.split()])\n    return text\n\n#And now let's apply this function to our text\nX = X.apply(lambda x: clean_data(x))\nX","metadata":{"id":"KOf68ReOfpb8","outputId":"d396fff4-56f7-4b03-d3f1-ec67c1c511ae"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now the text is clean!\nwordcloud = WordCloud(background_color='white', \n                      max_words=200,\n                      width=1500, \n                      height=800, colormap='twilight').generate(' '.join(X))\n\nplt.figure(figsize=(32,8))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","metadata":{"id":"tSHKw4aLgGoh","outputId":"02acfd52-15fb-4967-926d-294c22668574"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now to the feature engineering part**","metadata":{"id":"7yvLHMMeXBwm"}},{"cell_type":"markdown","source":"*   **What is TF-IDF and why we use it?**\n---\nüìå Term frequency-inverse document frequency. \nMachine learning algorithms often use numerical data, so when dealing with textual data or any natural language processing (NLP) task, a sub-field of ML/AI dealing with text, that data first needs to be converted to a vector of numerical data by a process known as vectorization. TF-IDF vectorization involves calculating the TF-IDF score for every word in your corpus relative to that document and then putting that information into a vector (see image below using example documents ‚ÄúA‚Äù and ‚ÄúB‚Äù). Thus each document in your corpus would have its own vector, and the vector would have a TF-IDF score for every single word in the entire collection of documents. Once you have these vectors you can apply them to various use cases such as seeing if two documents are similar by comparing their TF-IDF vector using cosine similarity.\n\n*   **What is PCA and why we use it?**\n\n---\nüìå Principal component analysis. Principal component analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n\nReducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\n\nSo, to sum up, the idea of PCA is simple ‚Äî reduce the number of variables of a data set, while preserving as much information as possible.\nWhy we use it?\n\n1.   Standardize the range of continuous initial variables\n2.   Compute the covariance matrix to identify correlations\n3.   Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components\n4.   Create a feature vector to decide which principal components to keep\nRecast the data along the principal components axes\n\n\n\n\n\n\n\n\n","metadata":{"id":"KqirgK6sX4Ln"}},{"cell_type":"code","source":"#I used the TFIDF vectorizer to vectorize my text\ntfidf_vec = TfidfVectorizer(min_df=0.005, max_features=5000) \ndata_tfidf = tfidf_vec.fit_transform(X).todense()\n\n#And now I will use the PCA \npca = PCA( 0.95, random_state=42)\ndata_pca = pca.fit_transform(data_tfidf)","metadata":{"id":"gZ86sdAJ2uXf","outputId":"8ba3c74e-2b3d-4a84-ddde-662a7529568d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We need to check the shape of our array\nprint( f\"TF-IDF dimension - {data_tfidf.shape[1]}\" )\nprint( f\"TF-IDF + PCA dimension - {data_pca.shape[1]}\" )","metadata":{"id":"7m0dD7td3P-u","outputId":"a15493f0-0e3d-45ba-f52a-4bd2dd618c5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now I will identify the number of words so I can get the features and make all the featuers equal and the same dimesion\nN_WORDS = 30\nmean_data_tfidf = np.array(data_tfidf.mean(axis=0)).flatten()\nvocabulary = tfidf_vec.get_feature_names()\nwords_id = np.flip( mean_data_tfidf.argsort()[-N_WORDS:] )\n\n#Now let's build our features dataframe\nword_val_data = [(vocabulary[id], mean_data_tfidf[id]) for id in words_id]\nword_val_data = pd.DataFrame(word_val_data, columns=['words','values'])","metadata":{"id":"iQDyaNIa3W71","outputId":"68111bac-1bfe-49b6-ce3b-961737a25e6c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3- (5 points)**","metadata":{"id":"Rm-0rAc1dXgC"}},{"cell_type":"markdown","source":"**I will use the KMeans algorithm**","metadata":{"id":"23DuDqqTdeR6"}},{"cell_type":"markdown","source":"**K-Means Clustering**\n\n---\n\n\nK-means clustering is the most commonly used unsupervised machine learning algorithm for partitioning a given data set into a set of k groups (i.e. k clusters), where k represents the number of groups pre-specified by the analyst. It classifies objects in multiple groups (i.e., clusters), such that objects within the same cluster are as similar as possible (i.e., high intra-class similarity), whereas objects from different clusters are as dissimilar as possible (i.e., low inter-class similarity). In k-means clustering, each cluster is represented by its center (i.e, centroid) which corresponds to the mean of points assigned to the cluster.\n\n\n---\n\n\n**The Basic Idea**\n\n---\n\n\nThe basic idea behind k-means clustering consists of defining clusters so that the total intra-cluster variation (known as total within-cluster variation) is minimized. There are several k-means algorithms available. The standard algorithm is the Hartigan-Wong algorithm (1979), which defines the total within-cluster variation as the sum of squared distances Euclidean distances between items and the corresponding centroid:\n![k-means-objective-function.png](https://www.researchgate.net/profile/Mahesh-Sarathchandra/publication/344888655/figure/fig1/AS:950914114408448@1603727000307/k-means-objective-function.ppm)","metadata":{"id":"xfp9KdtreLeI"}},{"cell_type":"markdown","source":"# **Modeling**","metadata":{"id":"KfGEBHbLgAMs"}},{"cell_type":"markdown","source":"**Now I will explain my Hyper-parameters and why did I choose them**","metadata":{"id":"zV56MX85ggQb"}},{"cell_type":"markdown","source":"\n\n*   n_init\n\n---\nIn K-means the initial placement of centroid plays a very important role in it's convergence. Sometimes, the initial centroids are placed in a such a way that during consecutive iterations of K-means the clusters the clusters keep on changing drastically and even before the convergence condition may occur, max_iter is reached and we are left with incorrect cluster. Hence, the clusters obtained in such may not be correct. To overcome this problem, this parameter is introduced. The value of n_iter basically determines how many different sets of randomly chosen centroids, should the algorithm use. For each different set of points, a comparision is made about how much distance did the clusters move, i.e. if the clusters travelled small distances than it is highly likely that we are closest to ground truth/best solution. The points which provide the best performance and their respective run along with all the cluster labels are returned.\n\n**And because our clusers will be a little huge becase the data is huge so I choosed 30 as initial placement of centroid**\n\n\n\n*   n_clusters\n\n---\n\n**I will use a way to tune the number of clusters and then we will know what is the most optimal number of clusters we shall use.**\n\n","metadata":{"id":"EBnrp_uIiQxP"}},{"cell_type":"code","source":"#So let's tune between the range 90 - 105\ncluster_sizes = range(90, 105)\nkmeans_models = [KMeansTF(i, n_init=30) for i in cluster_sizes]\ncluster_score = []\n\nfor kmeans in kmeans_models:\n  y = kmeans.fit_predict(data_pca)\n  score = silhouette_score(data_pca, y)\n  cluster_score.append((kmeans.n_clusters, score))\n\ncluster_score=np.array(cluster_score)","metadata":{"id":"ZysEBjEe3uxs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nscores = cluster_score[:, 1]\nclusters = cluster_score[:, 0]\nmax_score_clusters = []\n\nfig, ax = plt.subplots(figsize=(8,5))\nax = sns.lineplot(x=clusters, y=scores, ax=ax)\nax.set_title(\"Silhouette score vs No. clusters\", fontsize=16)\n\nfor i in np.argsort(scores)[-5:]:\n  ax.vlines(clusters[i], 0, 1, linestyles='--', colors='orange')\n  max_score_clusters.append(clusters[i])\n\nax.text(1.01, 1, f\"Dashed lines indicate\\n the {len(max_score_clusters)} highest scores\",\n        transform=ax.transAxes, ha='left', va='top')\n\nxticks = ax.get_xticks().astype(int)\nxticks = np.append(xticks, max_score_clusters)\nax.set_xticks( xticks )\nax.tick_params(axis='x', rotation=45)\n\nax.set_ylim([0.95*min(scores), 1.05*max(scores)])\nax.set_xlim()\n\nplt.show()","metadata":{"id":"0zL2lEVV4k3D","outputId":"4af44e8d-3a64-4f9d-a831-6a3f4624178f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So as we see the optimal number of clusters will be 103**","metadata":{"id":"KNNaAjAgmASb"}},{"cell_type":"markdown","source":"**Now let's go and apply it to our data**","metadata":{"id":"6WmQvqJSmP5P"}},{"cell_type":"code","source":"n_clusters = 103\nkmeans_model = KMeans(n_clusters, n_init=30)\ny = kmeans_model.fit_predict(data_pca)","metadata":{"id":"LpCTTo5NJ5hd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Clustering score and visualization**","metadata":{"id":"BhHkcZ2mmvU_"}},{"cell_type":"code","source":"#Now let's see a sample of our result\nsample_scores = silhouette_samples( data_pca, y )\nsample_scores_df =  pd.DataFrame( data = {'Cluster':y, 'Silhouette':sample_scores} )\nsample_scores_df = sample_scores_df.reset_index()\nsample_scores_df=sample_scores_df.sort_values('Silhouette', ascending=False)\nsample_scores_df","metadata":{"id":"qkYOG72XKJ0C","outputId":"5745920e-8235-4b1a-d395-5363e20dc960"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's visualize the clustering results**","metadata":{"id":"nLA7Ma62m2Au"}},{"cell_type":"code","source":"def plot_silhouette_samples(X, pred_labels):\n  n_clusters = len(np.unique(pred_labels))\n\n  fig, (ax) = plt.subplots(1, 1, figsize=(8,15))\n  \n\n  silhouette_avg = silhouette_score(X, pred_labels)\n  sample_silhouette_values = silhouette_samples(X, pred_labels)\n\n  y_lower = 10\n  for i in range(n_clusters):\n    ith_cluster_silhouette_values = sample_silhouette_values[pred_labels == i]\n    ith_cluster_silhouette_values.sort()\n\n    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n    y_upper = y_lower + size_cluster_i\n  \n    ax.fill_betweenx(np.arange(y_lower, y_upper), \n                     0, ith_cluster_silhouette_values)\n    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i),\n            ha='center', va='center', fontsize=12,\n            bbox={'boxstyle':'square',\n                  'facecolor':'white'})\n    y_lower = y_upper + 10\n  \n  ax.set_title(f\"The silhouette score plot for the {n_clusters} clusters.\\n\",fontsize=20)\n  ax.set_xlabel(\"Silhouette coefficient values\",fontsize=18)\n  ax.set_ylabel(\"Cluster\",fontsize=18)\n  ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n  ax.set_yticks([])\n  ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n  return ax\n\nplot_silhouette_samples(data_pca, y)","metadata":{"id":"DTZfradn2Azn","outputId":"e71d1309-3a8a-4e5e-b472-0c5a2a57a423"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now let's see some visuals to see the most important words in every cluster**","metadata":{"id":"eVrePekonGY-"}},{"cell_type":"code","source":"#Let's see\ndef most_important_words(data_tfidf, y, topn=10):\n\n  n_clusters = len(np.unique(y))\n  result = []\n  for i in range(n_clusters):\n    ith_cluter_word_sum = np.mean(data_tfidf[np.argwhere(y==i).flatten()], axis=0)\n    \n    word_ids = np.array(np.argsort( ith_cluter_word_sum )[0, -topn:])\n    word_ids = word_ids.reshape(-1)\n    result = result + [(i, id, ith_cluter_word_sum[0, id]) for id in word_ids]\n\n  return result","metadata":{"id":"PUJAtR1NOZdo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_cluster_df = pd.DataFrame(most_important_words(data_tfidf, y, topn=5), columns=[\"Cluster\", \"WordId\", \"Score\"])\nword_cluster_df[\"Word\"] = word_cluster_df[\"WordId\"].apply(lambda id: tfidf_vec.get_feature_names()[id])\nword_cluster_df = word_cluster_df.sort_values(\"Score\", ascending=False)\nword_cluster_df.head()","metadata":{"id":"kyJc6PSUOpl3","outputId":"889a9350-d27b-4fd9-f897-72d2fd35ec7f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.catplot(x=\"Score\", y=\"Word\", col=\"Cluster\", data=word_cluster_df, \n                sharey=False, col_wrap=4, kind=\"bar\",\n                color = 'red', aspect=.6)\n[ax.tick_params(axis='x', rotation=45, size=13) for ax in g.axes.flatten()]\ng.fig.suptitle(\"Words with highest TF-IDF scores in each cluster\", y = 1.01, fontsize=15)\nplt.show()","metadata":{"id":"kF4a4d15Osi8","outputId":"edb7aa4d-874c-4c6a-9016-39d9e81d57d7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Thank you for your reading!**","metadata":{"id":"QTSnfpdMo6yR"}}]}